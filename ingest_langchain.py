# ============================================================
# ingest_langchain.py
#
# ì´ íŒŒì¼ì˜ ì—­í•  (RAGì—ì„œ ë§¤ìš° ì¤‘ìš”)
# ------------------------------------------------------------
# âœ” docs í´ë” ì•ˆì˜ ë‹¤ì–‘í•œ ë¬¸ì„œ íŒŒì¼ì„ ì½ëŠ”ë‹¤
# âœ” ëª¨ë“  íŒŒì¼ì„ LangChainì˜ Document í˜•íƒœë¡œ í†µì¼í•œë‹¤
# âœ” ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ chunkë¡œ ìª¼ê° ë‹¤
# âœ” ê° chunkë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ì„œ Chroma ë²¡í„°DBì— ì €ì¥í•œë‹¤
#
# ì¦‰,
# ğŸ‘‰ "RAGì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°"ë¥¼ ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘ëŠ” ë‹¨ê³„
#
# ì‹¤í–‰:
#   python ingest_langchain.py
#
# ì£¼ì˜:
# - ì„œë²„ ì½”ë“œê°€ ì•„ë‹˜
# - API ì½”ë“œê°€ ì•„ë‹˜
# - ë¬¸ì„œê°€ ë°”ë€Œì—ˆì„ ë•Œë§Œ ì‹¤í–‰í•˜ë©´ ë¨
# ============================================================


# ----------------------------
# íŒŒì´ì¬ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ----------------------------
import os      # ê²½ë¡œ ì²˜ë¦¬, í´ë” ìƒì„±
import glob    # í´ë” ì•ˆ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰


# ----------------------------
# LangChain í•µì‹¬ ìë£Œêµ¬ì¡°
# ----------------------------

# Document:
# - LangChainì—ì„œ ì‚¬ìš©í•˜ëŠ” "ë¬¸ì„œ í‘œì¤€ í˜•íƒœ"
# - page_content : ì‹¤ì œ í…ìŠ¤íŠ¸
# - metadata     : ì¶œì²˜, í˜ì´ì§€ ë²ˆí˜¸, ê¸°íƒ€ ì •ë³´
from langchain_core.documents import Document


# RecursiveCharacterTextSplitter:
# - ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì€ ë‹¨ìœ„(chunk)ë¡œ ìª¼ê°œëŠ” ë„êµ¬
from langchain_text_splitters import RecursiveCharacterTextSplitter


# OpenAIEmbeddings:
# - í…ìŠ¤íŠ¸ â†’ ìˆ«ì ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜
from langchain_openai import OpenAIEmbeddings


# Chroma:
# - ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ë²¡í„°DB
from langchain_chroma import Chroma


# ----------------------------
# ë¬¸ì„œ ë¡œë”ë“¤ (íŒŒì¼ íƒ€ì…ë³„)
# ----------------------------
# ê° íŒŒì¼ì„ ì½ì–´ì„œ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• 
from langchain_community.document_loaders import (
    TextLoader,                 # .txt
    UnstructuredMarkdownLoader, # .md
    PyPDFLoader,                # .pdf
    Docx2txtLoader,             # .docx
    BSHTMLLoader,               # .html / .htm
)


# ----------------------------
# í”„ë¡œì íŠ¸ ê³µí†µ ì„¤ì •
# ----------------------------
# config.pyì— ì •ì˜ëœ ê°’ë“¤
from config import EMBED_MODEL, OPENAI_API_KEY


# ============================================================
# ì´ íŒŒì¼ ì „ìš© ì„¤ì •ê°’
# ============================================================

# ë¬¸ì„œê°€ ë“¤ì–´ìˆëŠ” í´ë”
DOCS_DIR = "./docs"

# ë²¡í„°DBê°€ ì €ì¥ë  í´ë”
CHROMA_DIR = "./chroma_db"

# Chroma ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¬ë ‰ì…˜ ì´ë¦„
COLLECTION_NAME = "my_rag_docs"

# chunk í¬ê¸°
# - ë„ˆë¬´ í¬ë©´ ê²€ìƒ‰ì´ ë‘”í•´ì§
# - ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ì´ ëŠê¹€
CHUNK_SIZE = 1500

# chunk ê²¹ì¹¨ ì˜ì—­
# - ì•/ë’¤ ë¬¸ë§¥ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ ì¼ë¶€ ê²¹ì¹¨
CHUNK_OVERLAP = 150


# ============================================================
# 1ï¸âƒ£ ë¬¸ì„œ ë¡œë”© ë‹¨ê³„ (ë¡œë” í™•ì¥ ë²„ì „)
# ============================================================
def load_docs_from_folder(folder: str) -> list[Document]:
    """
    docs í´ë” ì•ˆì˜ íŒŒì¼ë“¤ì„ í™•ì¥ìë³„ ë¡œë”ë¡œ ì½ì–´ì„œ
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    ì§€ì› í™•ì¥ì:
    - .txt
    - .md
    - .pdf
    - .docx
    - .html / .htm
    """

    docs: list[Document] = []

    # (í™•ì¥ì, ë¡œë” ìƒì„± í•¨ìˆ˜) ë§¤í•‘
    # ìƒˆë¡œìš´ íŒŒì¼ íƒ€ì…ì„ ì¶”ê°€í•˜ê³  ì‹¶ìœ¼ë©´
    # ì—¬ê¸° í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ ë¨
    loader_rules = [
        (".txt",  lambda p: TextLoader(p, encoding="utf-8")),
        (".md",   lambda p: TextLoader(p, encoding="utf-8")),
        (".pdf",  lambda p: PyPDFLoader(p)),
        (".docx", lambda p: Docx2txtLoader(p)),
        (".html", lambda p: BSHTMLLoader(p)),
        (".htm",  lambda p: BSHTMLLoader(p)),
    ]

    # docs í´ë” ì•„ë˜ ëª¨ë“  íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
    for path in glob.glob(os.path.join(folder, "**/*"), recursive=True):

        # íŒŒì¼ì´ ì•„ë‹ˆë©´(í´ë”ë©´) ë¬´ì‹œ
        if not os.path.isfile(path):
            continue

        # í™•ì¥ì ì¶”ì¶œ (.pdf, .txt ë“±)
        ext = os.path.splitext(path)[1].lower()

        # í™•ì¥ìì— ë§ëŠ” ë¡œë” ì°¾ê¸°
        for rule_ext, make_loader in loader_rules:
            if ext == rule_ext:
                try:
                    # ë¡œë” ìƒì„±
                    loader = make_loader(path)

                    # íŒŒì¼ì„ ì½ì–´ì„œ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    loaded_docs = loader.load()

                    # source ë©”íƒ€ë°ì´í„°ë¥¼ "íŒŒì¼ ê²½ë¡œ"ë¡œ í†µì¼
                    abs_path = os.path.abspath(path)
                    for d in loaded_docs:
                        d.metadata["source"] = abs_path

                    # ê²°ê³¼ ëˆ„ì 
                    docs.extend(loaded_docs)

                except Exception as e:
                    # íŒŒì¼ í•˜ë‚˜ê°€ ê¹¨ì ¸ ìˆì–´ë„ ì „ì²´ ingestê°€ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
                    print(f"[WARN] failed to load: {path} ({e})")

                # ë¡œë” ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ
                break

    return docs


# ============================================================
# 2ï¸âƒ£ ì²­í‚¹ ë‹¨ê³„
# ============================================================
def chunk_docs(docs: list[Document]) -> list[Document]:
    """
    ê¸´ Documentë“¤ì„ ì‘ì€ chunk Documentë“¤ë¡œ ìª¼ê° ë‹¤
    """

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )

    # ì…ë ¥  : [Document, Document, ...]
    # ì¶œë ¥  : [chunked Document, chunked Document, ...]
    return splitter.split_documents(docs)


# ============================================================
# 3ï¸âƒ£ ë²¡í„°DB ì €ì¥ ë‹¨ê³„
# ============================================================
def build_or_update_chroma(chunks: list[Document]) -> None:
    """
    chunk Documentë“¤ì„ ì„ë² ë”©í•´ì„œ
    Chroma ë²¡í„°DBì— ì €ì¥í•œë‹¤
    """

    # ì„ë² ë”© ê°ì²´ ìƒì„±
    embeddings = OpenAIEmbeddings(
        model=EMBED_MODEL,
        api_key=OPENAI_API_KEY
    )

    # Chroma ë²¡í„°DB ë¡œë“œ ë˜ëŠ” ìƒì„±
    # persist_directoryì— ìë™ìœ¼ë¡œ íŒŒì¼ ì €ì¥ë¨
    db = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_DIR,
    )

    # chunk Documentë“¤ì„ ê·¸ëŒ€ë¡œ DBì— ì¶”ê°€
    db.add_documents(chunks)


# ============================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================
def main():
    """
    ingest_langchain.py ì‹¤í–‰ ì‹œ
    ì—¬ê¸°ë¶€í„° ì‹œì‘ëœë‹¤
    """

    # docs / chroma_db í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(DOCS_DIR, exist_ok=True)
    os.makedirs(CHROMA_DIR, exist_ok=True)

    # 1) ë¬¸ì„œ ë¡œë”©
    docs = load_docs_from_folder(DOCS_DIR)
    if not docs:
        print(f"[WARN] no docs found in {DOCS_DIR}")
        return

    # 2) ì²­í‚¹
    chunks = chunk_docs(docs)

    # 3) ë²¡í„°DB ì €ì¥
    build_or_update_chroma(chunks)

    print(f"[OK] loaded docs: {len(docs)}, stored chunks: {len(chunks)}")


# ============================================================
# íŒŒì´ì¬ íŒŒì¼ ì§ì ‘ ì‹¤í–‰ ì‹œ ì‹œì‘ ì§€ì 
# ============================================================
if __name__ == "__main__":
    main()
